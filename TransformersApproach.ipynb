{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmOcirvdWw2RwklYyr5zDK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mxag11z/EMO/blob/main/TransformersApproach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers approach:\n",
        "\n",
        "Estrategia 3: entrenar un modelo multilingüe en el conjunto de entrenamiento en inglés y evaluarlo directamente en el conjunto de evaluación en español."
      ],
      "metadata": {
        "id": "L7slf9G-v9O9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0L6vnKd2uhz",
        "outputId": "c4a07c9a-17b7-4c26-af5f-59f62480bc46"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZoT-NYWev8Ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments,AdamW,get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.float)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "vqGHux2mx0mn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, device, epochs=3):\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Configurar el scheduler\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "\n",
        "    # Criterio MSE para regresión\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(train_dataloader, desc='Training'):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits.squeeze()\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_dataloader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        predictions = []\n",
        "        true_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc='Validation'):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits.squeeze()\n",
        "\n",
        "                loss = criterion(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                predictions.extend(logits.cpu().numpy())\n",
        "                true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        val_mae = mean_absolute_error(true_labels, predictions)\n",
        "\n",
        "        print(f'Average training loss: {avg_train_loss:.4f}')\n",
        "        print(f'Average validation loss: {avg_val_loss:.4f}')\n",
        "        print(f'Validation MAE: {val_mae:.4f}')\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "\n",
        "    return val_mae"
      ],
      "metadata": {
        "id": "GBD_AaPrzsaL"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d3Iai8n0v8sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels):\n",
        "    \"\"\"\n",
        "    Compara diferentes modelos usando train, dev y test sets\n",
        "    \"\"\"\n",
        "    # Definir los modelos a comparar\n",
        "    models = {\n",
        "        'mBERT': 'bert-base-multilingual-cased',\n",
        "        'XLM-RoBERTa': 'xlm-roberta-base',\n",
        "        'mT5': 'google/mt5-base'\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    for model_name, model_path in models.items():\n",
        "        print(f'\\nEvaluating {model_name}...')\n",
        "\n",
        "        # Cargar tokenizer y modelo\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            model_path,\n",
        "            num_labels=1  # Regresión: una salida continua\n",
        "        ).to(device)\n",
        "\n",
        "        # Crear datasets\n",
        "        train_dataset = EmotionDataset(train_texts, train_labels, tokenizer)\n",
        "        dev_dataset = EmotionDataset(dev_texts, dev_labels, tokenizer)\n",
        "        test_dataset = EmotionDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "        # Crear dataloaders\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "        dev_dataloader = DataLoader(dev_dataset, batch_size=16)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "        # Entrenar usando dev para validación\n",
        "        train_mae = train_model(model, train_dataloader, dev_dataloader, device)\n",
        "        print(f\"Training MAE: {train_mae:.4f}\")\n",
        "\n",
        "        # Evaluar en test\n",
        "        test_mae = evaluate_model(model, test_dataloader, device)\n",
        "        print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "        results[model_name] = {\n",
        "            'train_mae': train_mae,\n",
        "            'test_mae': test_mae\n",
        "        }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Lp01n_YI2C_5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(emotion):\n",
        "    \"\"\"\n",
        "    Lee los datos de train, dev y test para una emoción\n",
        "    \"\"\"\n",
        "    # Datos de entrenamiento (inglés)\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/en/train/{emotion}.txt\", 'r', encoding='utf-8') as f:\n",
        "        train_X = f.readlines()\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/en/train/{emotion}_labels.txt\", 'r', encoding='utf-8') as f:\n",
        "        train_y = [float(line.strip()) for line in f.readlines()]\n",
        "\n",
        "    # Datos de validación (dev)\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/en/dev/{emotion}.txt\", 'r', encoding='utf-8') as f:\n",
        "        dev_X = f.readlines()\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/en/dev/{emotion}_labels.txt\", 'r', encoding='utf-8') as f:\n",
        "        dev_y = [float(line.strip()) for line in f.readlines()]\n",
        "\n",
        "    # Datos de test (español)\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/es/test/{emotion}.txt\", 'r', encoding='utf-8') as f:\n",
        "        test_X = f.readlines()\n",
        "    with open(f\"/content/drive/MyDrive/PLN project/data/es/test/{emotion}_labels.txt\", 'r', encoding='utf-8') as f:\n",
        "        test_y = [float(line.strip()) for line in f.readlines()]\n",
        "\n",
        "    return train_X, train_y, dev_X, dev_y, test_X, test_y"
      ],
      "metadata": {
        "id": "GHEXpSCM2ExC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_all_emotions():\n",
        "    \"\"\"\n",
        "    Evalúa todos los modelos en todas las emociones\n",
        "    \"\"\"\n",
        "    emotions = ['joy', 'anger', 'sadness', 'fear']\n",
        "    all_results = {}\n",
        "\n",
        "    for emotion in emotions:\n",
        "        print(f\"\\n=== Evaluating {emotion} ===\")\n",
        "        # Cargar datos incluyendo dev set\n",
        "        train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels = read_data(emotion)\n",
        "\n",
        "        # Comparar modelos\n",
        "        results = compare_models(\n",
        "            train_texts, train_labels,\n",
        "            dev_texts, dev_labels,\n",
        "            test_texts, test_labels\n",
        "        )\n",
        "        all_results[emotion] = results\n",
        "\n",
        "        # Mostrar resultados para esta emoción\n",
        "        print(f\"\\nResults for {emotion}:\")\n",
        "        for model_name, scores in results.items():\n",
        "            print(f\"{model_name}:\")\n",
        "            print(f\"  Training MAE = {scores['train_mae']:.4f}\")\n",
        "            print(f\"  Test MAE = {scores['test_mae']:.4f}\")\n",
        "\n",
        "    return all_results"
      ],
      "metadata": {
        "id": "OEygj5xB2Kyy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evalúa el modelo en un conjunto de datos\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions.extend(outputs.logits.squeeze().cpu().numpy())\n",
        "            actual_labels.extend(labels.numpy())\n",
        "\n",
        "    return mean_absolute_error(actual_labels, predictions)"
      ],
      "metadata": {
        "id": "clIPR4I358Ct"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = evaluate_all_emotions()\n",
        "\n",
        "# Mostrar resultados completos\n",
        "print(\"\\n=== RESULTADOS FINALES ===\")\n",
        "for emotion in all_results:\n",
        "    print(f\"\\n{emotion.upper()}:\")\n",
        "    for model_name, mae in all_results[emotion].items():\n",
        "        print(f\"{model_name}: MAE = {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HdRrZ5u22iuE",
        "outputId": "e80b2948-7f16-40e6-8cda-5821a161bbc9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating joy ===\n",
            "\n",
            "Evaluating mBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:17<00:00,  2.96it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0549\n",
            "Average validation loss: 0.0351\n",
            "Validation MAE: 0.1561\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:17<00:00,  2.93it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00,  9.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0317\n",
            "Average validation loss: 0.0276\n",
            "Validation MAE: 0.1308\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:18<00:00,  2.87it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00,  9.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0227\n",
            "Average validation loss: 0.0199\n",
            "Validation MAE: 0.1131\n",
            "Training MAE: 0.1131\n",
            "Test MAE: 0.2335\n",
            "\n",
            "Evaluating XLM-RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:20<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1274\n",
            "Average validation loss: 0.0453\n",
            "Validation MAE: 0.1763\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:20<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0510\n",
            "Average validation loss: 0.0271\n",
            "Validation MAE: 0.1329\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:20<00:00,  2.56it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0328\n",
            "Average validation loss: 0.0214\n",
            "Validation MAE: 0.1165\n",
            "Training MAE: 0.1165\n",
            "Test MAE: 0.1894\n",
            "\n",
            "Evaluating mT5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:48<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.3144\n",
            "Average validation loss: 0.1358\n",
            "Validation MAE: 0.3045\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:48<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.2023\n",
            "Average validation loss: 0.1306\n",
            "Validation MAE: 0.2799\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 52/52 [00:48<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1934\n",
            "Average validation loss: 0.1168\n",
            "Validation MAE: 0.2745\n",
            "Training MAE: 0.2745\n",
            "Test MAE: 0.3042\n",
            "\n",
            "Results for joy:\n",
            "mBERT:\n",
            "  Training MAE = 0.1131\n",
            "  Test MAE = 0.2335\n",
            "XLM-RoBERTa:\n",
            "  Training MAE = 0.1165\n",
            "  Test MAE = 0.1894\n",
            "mT5:\n",
            "  Training MAE = 0.2745\n",
            "  Test MAE = 0.3042\n",
            "\n",
            "=== Evaluating anger ===\n",
            "\n",
            "Evaluating mBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:19<00:00,  2.84it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0530\n",
            "Average validation loss: 0.0236\n",
            "Validation MAE: 0.1175\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:18<00:00,  2.84it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0260\n",
            "Average validation loss: 0.0198\n",
            "Validation MAE: 0.1071\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:19<00:00,  2.84it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 10.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0150\n",
            "Average validation loss: 0.0205\n",
            "Validation MAE: 0.1103\n",
            "Training MAE: 0.1103\n",
            "Test MAE: 0.2393\n",
            "\n",
            "Evaluating XLM-RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:21<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 11.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0577\n",
            "Average validation loss: 0.0240\n",
            "Validation MAE: 0.1157\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:21<00:00,  2.55it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0340\n",
            "Average validation loss: 0.0231\n",
            "Validation MAE: 0.1158\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:21<00:00,  2.55it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:00<00:00, 11.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0277\n",
            "Average validation loss: 0.0181\n",
            "Validation MAE: 0.1015\n",
            "Training MAE: 0.1015\n",
            "Test MAE: 0.2009\n",
            "\n",
            "Evaluating mT5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:50<00:00,  1.07it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:01<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1548\n",
            "Average validation loss: 0.0501\n",
            "Validation MAE: 0.1705\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:50<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:01<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1416\n",
            "Average validation loss: 0.0622\n",
            "Validation MAE: 0.1958\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 54/54 [00:50<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 6/6 [00:01<00:00,  4.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1469\n",
            "Average validation loss: 0.0527\n",
            "Validation MAE: 0.1736\n",
            "Training MAE: 0.1736\n",
            "Test MAE: 0.2592\n",
            "\n",
            "Results for anger:\n",
            "mBERT:\n",
            "  Training MAE = 0.1103\n",
            "  Test MAE = 0.2393\n",
            "XLM-RoBERTa:\n",
            "  Training MAE = 0.1015\n",
            "  Test MAE = 0.2009\n",
            "mT5:\n",
            "  Training MAE = 0.1736\n",
            "  Test MAE = 0.2592\n",
            "\n",
            "=== Evaluating sadness ===\n",
            "\n",
            "Evaluating mBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0676\n",
            "Average validation loss: 0.0305\n",
            "Validation MAE: 0.1465\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0419\n",
            "Average validation loss: 0.0300\n",
            "Validation MAE: 0.1485\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:17<00:00,  2.86it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00,  9.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0335\n",
            "Average validation loss: 0.0269\n",
            "Validation MAE: 0.1396\n",
            "Training MAE: 0.1396\n",
            "Test MAE: 0.2220\n",
            "\n",
            "Evaluating XLM-RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:19<00:00,  2.57it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0866\n",
            "Average validation loss: 0.0390\n",
            "Validation MAE: 0.1646\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:19<00:00,  2.56it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0472\n",
            "Average validation loss: 0.0221\n",
            "Validation MAE: 0.1165\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:19<00:00,  2.57it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:00<00:00, 10.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0400\n",
            "Average validation loss: 0.0233\n",
            "Validation MAE: 0.1204\n",
            "Training MAE: 0.1204\n",
            "Test MAE: 0.2098\n",
            "\n",
            "Evaluating mT5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:46<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.3183\n",
            "Average validation loss: 0.1348\n",
            "Validation MAE: 0.3102\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:46<00:00,  1.09it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1709\n",
            "Average validation loss: 0.1065\n",
            "Validation MAE: 0.2636\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 50/50 [00:46<00:00,  1.08it/s]\n",
            "Validation: 100%|██████████| 5/5 [00:01<00:00,  3.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1432\n",
            "Average validation loss: 0.1011\n",
            "Validation MAE: 0.2461\n",
            "Training MAE: 0.2461\n",
            "Test MAE: 0.2881\n",
            "\n",
            "Results for sadness:\n",
            "mBERT:\n",
            "  Training MAE = 0.1396\n",
            "  Test MAE = 0.2220\n",
            "XLM-RoBERTa:\n",
            "  Training MAE = 0.1204\n",
            "  Test MAE = 0.2098\n",
            "mT5:\n",
            "  Training MAE = 0.2461\n",
            "  Test MAE = 0.2881\n",
            "\n",
            "=== Evaluating fear ===\n",
            "\n",
            "Evaluating mBERT...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:25<00:00,  2.83it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00,  9.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0502\n",
            "Average validation loss: 0.0240\n",
            "Validation MAE: 0.1287\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:25<00:00,  2.82it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0259\n",
            "Average validation loss: 0.0178\n",
            "Validation MAE: 0.1082\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:25<00:00,  2.84it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0162\n",
            "Average validation loss: 0.0165\n",
            "Validation MAE: 0.1022\n",
            "Training MAE: 0.1022\n",
            "Test MAE: 0.2360\n",
            "\n",
            "Evaluating XLM-RoBERTa...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:28<00:00,  2.53it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 10.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0663\n",
            "Average validation loss: 0.0274\n",
            "Validation MAE: 0.1377\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:28<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0366\n",
            "Average validation loss: 0.0249\n",
            "Validation MAE: 0.1271\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [00:28<00:00,  2.54it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:00<00:00, 10.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.0293\n",
            "Average validation loss: 0.0202\n",
            "Validation MAE: 0.1153\n",
            "Training MAE: 0.1153\n",
            "Test MAE: 0.1986\n",
            "\n",
            "Evaluating mT5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Some weights of MT5ForSequenceClassification were not initialized from the model checkpoint at google/mt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [01:07<00:00,  1.07it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.7935\n",
            "Average validation loss: 0.1335\n",
            "Validation MAE: 0.2983\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [01:07<00:00,  1.07it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.2016\n",
            "Average validation loss: 0.1397\n",
            "Validation MAE: 0.2978\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 72/72 [01:07<00:00,  1.07it/s]\n",
            "Validation: 100%|██████████| 7/7 [00:01<00:00,  3.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average training loss: 0.1802\n",
            "Average validation loss: 0.1370\n",
            "Validation MAE: 0.2913\n",
            "Training MAE: 0.2913\n",
            "Test MAE: 0.3291\n",
            "\n",
            "Results for fear:\n",
            "mBERT:\n",
            "  Training MAE = 0.1022\n",
            "  Test MAE = 0.2360\n",
            "XLM-RoBERTa:\n",
            "  Training MAE = 0.1153\n",
            "  Test MAE = 0.1986\n",
            "mT5:\n",
            "  Training MAE = 0.2913\n",
            "  Test MAE = 0.3291\n",
            "\n",
            "=== RESULTADOS FINALES ===\n",
            "\n",
            "JOY:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unsupported format string passed to dict.__format__",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-7e44d8d6c8d7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{emotion.upper()}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_name}: MAE = {mae:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to dict.__format__"
          ]
        }
      ]
    }
  ]
}